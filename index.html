<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Stolen from Sergey and Jon Barron */
  a {
    color: #1772d0;
    text-decoration:none;
  }
  a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
  }
  body,td,th {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px
  }
  strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;


    font-size: 14px
  }
  strongred {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    color: 'red'

    font-size: 14px
  }
  heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;*/


    font-size: 15px;
    font-weight: 700
  }
</style>
<!-- <link rel="icon" type="image/png" href="seal_icon.png"> -->
<script type="text/javascript" src="hidebib.js"></script>
<title>Debidatta Dwibedi</title>
<meta name="Debidatta Dwibedi" http-equiv="Content-Type" content="Debidatta Dwibedi">

<link href="css" rel="stylesheet" type="text/css">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90857215-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>

  <table width="960" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td>
        <p align="center"><font size="7">Debidatta Dwibedi</font><br>
        </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="67%" valign="middle" align="justify">



              <p>I am a researcher in <a href="https://ai.google/research/teams/brain">Google Brain</a>. I completed my Masters in Robotics from the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> at <a href="https://www.cmu.edu/">CMU</a>, where I was advised by <a href="http://www.cs.cmu.edu/~hebert/"> Martial Hebert</a>. Prior to that I completed my undergrad from <a href="http://www.iitk.ac.in/">IIT Kanpur</a>, where I worked with <a href="http://www.cse.iitk.ac.in/users/amit/">Amitabha Mukerjee</a>.
                <br>

              </p><p align="center">
                <a href="mailto:debi.me@gmail.com">Email</a> &nbsp;/&nbsp;
                <!-- <a href="assets/DebidattaDwibedi_CV.pdf">CV</a> &nbsp;/&nbsp; -->
                <a href="https://scholar.google.com/citations?hl=en&user=EPfOJwQAAAAJ">Google Scholar</a> &nbsp;/&nbsp; 
                <a href="https://github.com/debidatta/"> GitHub</a>&nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/debidatta-dwibedi-0b115014"> LinkedIn </a>&nbsp;/&nbsp;
                <a href="https://www.twitter.com/debidatta/"> Twitter </a>
              </p>

            </p><p align="center">
              <a href="#publications">Publications</a> &nbsp;/&nbsp;
              <a href="#patents">Patents</a> &nbsp;/&nbsp;
              <a href="#talks">Talks</a>&nbsp;/&nbsp;
              <a href="#theses">Theses</a>&nbsp;/&nbsp;
              <a href="#misc">Misc</a>
            </p>

          </td>
          <td width="33%"><img src="assets/debi.jpg" width="90%" style="background-repeat: no-repeat;background-position: 50%;border-radius: 50%;width: 200px;height: 200px;"></td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



        <tbody><tr><td>
          <h2>Research</h2>

          I want to build intelligent agents that interact with our world in useful ways.
          <br/><br/>
          My research lies at the intersection of machine learning, computer vision and robotics. Presently, I am working on imitation learning from videos and investigating the role time can play in learning better visual models.
          <br/><br/>
          
        </td></tr>

      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tbody><tr><td>
          <h2 id="publications">Publications</h2></td></tr>

          <tr>
            <td width="42%" valign="top"><a href="assets/tcc.gif"><img src="assets/tcc.gif" width="100%" height="60%" style="border-style: none"></a>
            </td><td width="58%" valign="top">
              <p><a href="https://arxiv.org/abs/1904.07846" id="TCC">
                <heading>Temporal Cycle-Consistency Learning</heading></a><br>
                Debidatta Dwibedi, <a href="https://people.csail.mit.edu/yusuf/">Yusuf Aytar</a>, <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a>, <a href="https://sermanet.github.io/home/">Pierre Sermanet</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> <br>
                <em><a href="http://cvpr2019.thecvf.com/">Computer Vision and Pattern Recognition (CVPR) 2019 </a></em>
                <br>
                <br>
                Self-supervised representation learning based on temporal alignment for fine-grained video understanding tasks.<br>

              </p>

              <div class="paper" id="tcc">
                <a href="https://arxiv.org/abs/1904.07846">paper</a> |
                <a href="tcc_interactive_paper/index.html">interactive paper</a> |
                <a href="javascript:toggleblock(&#39;tcc_abs&#39;)">abstract</a> |
                <a shape="rect" href="javascript:togglebib(&#39;tcc&#39;)" class="togglebib">bibtex</a> |
                <a href="https://sites.google.com/view/temporal-cycle-consistency"> project </a> |
                <a href="assets/tcc_poster.pdf"> poster </a> |
                <a href="https://ai.googleblog.com/2019/08/video-understanding-using-temporal.html"> Google AI blogpost </a> |
                <a href="https://github.com/google-research/google-research/tree/master/tcc"> code </a> |
                <a href="https://colab.research.google.com/drive/1-JYJXKoRWKcQvw5Tlacteotewpd2Bkts"> colab </a>

                <p align="justify"> <i id="tcc_abs" style="display: none;"> We introduce a self-supervised representation learning method based on the task of temporal alignment between videos. The method trains a network using temporal cycle consistency (TCC), a differentiable cycle-consistency loss that can be used to find correspondences across time in multiple videos. The resulting per-frame embeddings can be used to align videos by simply matching frames using the nearest-neighbors in the learned embedding space. 

                To evaluate the power of the embeddings, we densely label the Pouring and Penn Action video datasets for action phases. We show that (i) the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and (ii) TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings are also used for a number of applications based on alignment (dense temporal correspondence) between video pairs, including transfer of metadata of synchronized modalities between videos (sounds, temporal semantic labels), synchronized playback of multiple videos, and anomaly detection.</i></p>

                <div style="white-space: pre-wrap; display: none;" class="bib">
                 @InProceedings{Dwibedi_2019_CVPR,
				author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
				title = {Temporal Cycle-Consistency Learning},
				booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
				month = {June},
				year = {2019}
				}
             </div>
           </div>
         </td>
       </tr>

          <tr>
            <td width="42%" valign="top"><a href="assets/dac.gif"><img src="assets/dac.gif" width="100%" height="60%" style="border-style: none"></a>
            </td><td width="58%" valign="top">
              <p><a href="https://arxiv.org/abs/1809.02925" id="DAC">
                <heading>Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning</heading></a><br>
                <a href="https://github.com/ikostrikov">Ilya Kostrikov</a>, <a href="https://kumarkrishna.github.io/">Kumar Krishna Agrawal</a> , Debidatta Dwibedi, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> , and <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a><br>
                <em><a href="https://www.iclr.cc/">International Conference on Learning Representations (ICLR) 2019 </a></em>
                <br>
                <br>
                Sample efficient imitation learning using off-policy updates and proper handling of terminal states.<br>

              </p>

              <div class="paper" id="dac">
                <a href="https://arxiv.org/abs/1809.02925">paper</a> |
                <a href="javascript:toggleblock(&#39;dac_abs&#39;)">abstract</a> |
                <a shape="rect" href="javascript:togglebib(&#39;dac&#39;)" class="togglebib">bibtex</a> |
                <a href="https://github.com/google-research/google-research/tree/master/dac">code</a> 

                <p align="justify"> <i id="dac_abs" style="display: none;"> We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.</i></p>

                <div style="white-space: pre-wrap; display: none;" class="bib">
                 @inproceedings{
                 kostrikov2018discriminatoractorcritic,
                 title={Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning},
                 author={Ilya Kostrikov and Kumar Krishna Agrawal and Debidatta Dwibedi and Sergey Levine and Jonathan Tompson},
                 booktitle={International Conference on Learning Representations},
                 year={2019},
                 url={https://openreview.net/forum?id=Hk4fpoA5Km},
               }
             </div>
           </div>
         </td>
       </tr>


       <tr>
        <td width="42%" valign="top"><a href="assets/cheetah.gif"><img src="assets/cheetah.gif" width="100%" height="60%" style="border-style: none"></a>
        </td><td width="58%" valign="top">
          <p><a href="https://arxiv.org/abs/1808.00928" id="CHEETAH">
            <heading>Learning  Actionable  Representations  from  Visual  Observations</heading></a><br>
            Debidatta Dwibedi, <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a>, Corey Lynch and <a href="https://sermanet.github.io/home/">Pierre Sermanet</a><br>
            <em><a href="https://www.iros2018.org/">International Conference on Intelligent Robots (IROS) 2018 </a></href> </em>
            <br>
            <br>
            Control agents from pixels by learning self-supervised representations from videos.<br>

          </p>

          <div class="paper" id="mftcn">
            <a href="https://arxiv.org/abs/1808.00928">paper</a> |
            <a href="javascript:toggleblock(&#39;mftcn_abs&#39;)">abstract</a> |
            <a shape="rect" href="javascript:togglebib(&#39;mftcn&#39;)" class="togglebib">bibtex</a> |
            <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a>


            <p align="justify"> <i id="mftcn_abs" style="display: none;"> In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline.</i> </p>

            <div style="white-space: pre-wrap; display: none;" class="bib">
             @inproceedings{dwibedi2018learning,
             author    = {Dwibedi, Debidatta and Tompson, Jonathan and Lynch, Corey and Sermanet, Pierre},
             title     = {Learning Actionable Representations from Visual Observations},
             booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
             pages     = {1577--1584},
             year      = {2018},
             organization = {IEEE},
             url       = {https://arxiv.org/abs/1808.00928}
           }
         </div>
       </div>
     </td>
   </tr>

   <tr>
    <td width="42%" valign="top"><a href="https://arxiv.org/abs/1708.01642"><img src="assets/cutpaste.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="top">
      <p><a href="https://arxiv.org/abs/1708.01642" id="CUTPASTE">
        <heading>Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</heading></a><br>
        Debidatta Dwibedi, <a href="http://imisra.github.io/">Ishan Misra</a> and <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a><br>
        <em><a href="http://iccv2017.thecvf.com/">International Conference on Computrer Vision (ICCV) 2017</a></em>
        <br>
        <br>
        Generate synthetic data for detecting objects in scenes.<br>

      </p>

      <div class="paper" id="cutpaste">
        <a href="https://arxiv.org/abs/1708.01642">paper</a> |
        <a href="javascript:toggleblock(&#39;cutpaste_abs&#39;)">abstract</a> |
        <a shape="rect" href="javascript:togglebib(&#39;cutpaste&#39;)" class="togglebib">bibtex</a> |
        <a href="https://github.com/debidatta/syndata-generation">code</a> |
        <a href="assets/cutpaste_poster.pdf">poster</a>


        <p align="justify"> <i id="cutpaste_abs" style="display: none;">A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data.</i> </p>

        <div style="white-space: pre-wrap; display: none;" class="bib">
          @InProceedings{Dwibedi_2017_ICCV,
          author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
          title = {Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection},
          booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
          month = {Oct},
          year = {2017}
        }
      </div>
    </div>
  </td>
</tr>

<tr>
  <td width="42%" valign="top"><a href="assets/cuboid.png"><img src="assets/cuboid.png" width="100%" style="border-style: none"></a>
  </td><td width="58%" valign="top">
    <p><a href="https://arxiv.org/abs/1611.10010v1" id="CUBOID">
      <heading>Deep Cuboid Detection: Beyond 2D Bounding Boxes</heading></a><br>
      Debidatta Dwibedi, <a href="http://people.csail.mit.edu/tomasz/">Tomasz Malisiewicz</a>, <a href="https://sites.google.com/site/vijaybacademichomepage/home">Vijay Badrinarayanan</a> and <a href="https://ai.google/research/people/AndrewRabinovich">Andrew Rabinovich</a><br>
      <em>Arxiv Preprint</em>, 2016
      <br>
      <br>
      Cuboid detector using deep learning: finds cuboids in scenes and localizes their corners.<br>

    </p>

    <div class="paper" id="cuboid">
      <a href="https://arxiv.org/abs/1611.10010v1">paper</a> |
      <a href="javascript:toggleblock(&#39;cuboid_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cuboid&#39;)" class="togglebib">bibtex</a>


      <p align="justify"> <i id="cuboid_abs" style="display: none;">We present a Deep Cuboid Detector which takes a consumer-quality RGB image of a cluttered scene and localizes all 3D cuboids (box-like objects). Contrary to classical approaches which fit a 3D model from low-level cues like corners, edges, and vanishing points, we propose an end-to-end deep learning system to detect cuboids across many semantic categories (e.g., ovens, shipping boxes, and furniture). We localize cuboids with a 2D bounding box, and simultaneously localize the cuboid's corners, effectively producing a 3D interpretation of box-like objects. We refine keypoints by pooling convolutional features iteratively, improving the baseline method significantly. Our deep learning cuboid detector is trained in an end-to-end fashion and is suitable for real-time applications in augmented reality (AR) and robotics.</i> </p>

      <div style="white-space: pre-wrap; display: none;" class="bib">
        @article{dwibedi2016deep,
        title={Deep cuboid detection: Beyond 2d bounding boxes},
        author={Dwibedi, Debidatta and Malisiewicz, Tomasz and Badrinarayanan, Vijay and Rabinovich, Andrew},
        journal={arXiv preprint arXiv:1611.10010},
        year={2016}
      }
    </div>
  </div>
</td>
</tr>

<tr>
  <td width="42%" valign="top"><a href="assets/semantic_graph.png"><img src="assets/semantic_graph.png" width="100%" style="border-style: none"></a>
  </td><td width="58%" valign="top">
    <p><a href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_23" id="PA13">
      <heading>Characterizing Predicate Arity and Spatial Structure for Inductive Learning of Game Rules</heading></a><br>
      Debidatta Dwibedi and <a href="http://www.cse.iitk.ac.in/users/amit/">Amitabha Mukerjee</a><br>
      <em><a href="http://profs.sci.univr.it/~cristanm/contact2014/">ECCV 2014 Workshop on Computer Vision + Ontology Applied Cross-Disciplinary Technologies 2014</a></em>
      <br>
      <br>
      Represent videos as dynamic graphs. Learn rules of games from observing people play games in Kinect videos.<br>

    </p>

    <div class="paper" id="pa13">
      <a href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_23">paper</a> |
      <a href="javascript:toggleblock(&#39;pa13_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;pa13&#39;)" class="togglebib">bibtex</a> |
      <a href="https://www.youtube.com/playlist?list=PLDd4RddCgh3ctcftujGoS6CzC47fipqWt">videos</a>


      <p align="justify"> <i id="pa13_abs" style="display: none;">Where do the predicates in a game ontology come from? We use RGBD vision to learn a) the spatial structure of a board, and b) the number of parameters in a move or transition. These are used to define state-transition predicates for a logical description of each game state. Given a set of videos for a game, we use an improved 3D multi-object tracking to obtain the positions of each piece in games such as 4-peg solitaire or Towers of Hanoi. The spatial positions occupied by pieces over the entire game is clustered, revealing the structure of the board. Each frame is represented as a Semantic Graph with edges encoding spatial relations between pieces. Changes in the graphs between game states reveal the structure of a “move”. Knowledge from spatial structure and semantic graphs is mapped to FOL descriptions of the moves and used in an Inductive Logic framework to infer the valid moves and other rules of the game. Discovered predicate structures and induced rules are demonstrated for several games with varying board layouts and move structures.</i> </p>

      <div style="white-space: pre-wrap; display: none;" class="bib">
        @inproceedings{dwibedi2014characterizing,
        title={Characterizing Predicate Arity and Spatial Structure for Inductive Learning of Game Rules},
        author={Dwibedi, Debidatta and Mukerjee, Amitabha},
        booktitle={European Conference on Computer Vision},
        pages={323--338},
        year={2014},
        organization={Springer}
      }
    </div>
  </div>
</td>
</tr>



</tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="patents">Patents</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="https://patents.google.com/patent/US20180137642A1/en"><img src="assets/cuboid2.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="https://patents.google.com/patent/US20180137642A1/en"><heading>Deep learning system for cuboid detection</heading></a><br>
      </p>

    </td>
  </tr>

</tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="talks">Talks</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="https://drive.google.com/file/d/1tq9PYHqCXqVZdBggjfjfRRoHCv1hoSCa/view"><img src="assets/tcc_luv.gif" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="https://drive.google.com/file/d/1tq9PYHqCXqVZdBggjfjfRRoHCv1hoSCa/view"><heading>Temporal Cycle-Consistency Learning</heading></a><br>
        <em><a href="https://sites.google.com/view/luv2019/home?#h.p_ctb9i6zzj_WP">Learning from Unlabeled Videos</a> at <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a></em>
      </p>
      <div class="paper" id="tcc_ws">
        <a href="https://drive.google.com/file/d/1tq9PYHqCXqVZdBggjfjfRRoHCv1hoSCa/view">paper</a> | <a href="assets/tcc_luv_slides.pdf">slides</a> 

    </div>
  </td>
</tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/bivu2018.pdf"><img src="assets/bivu_teaser.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/bivu2018.pdf"><heading>Temporal Reasoning in Videos Using Convolutional Gated Recurrent Units</heading></a><br>
        <em><a href="https://bivu2018.github.io/#program">2nd Workshop in Brave New Ideas in Video Understanding</a> at <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a></em>
      </p>
      <div class="paper" id="temporal_ws">
        <a href="assets/bivu2018.pdf">paper</a> | <a href="assets/bnivu_slides.pdf">slides</a> | <a href="assets/bnivu_poster.pdf">poster</a> | <a shape="rect" href="javascript:togglebib(&#39;temporal_ws&#39;)" class="togglebib">bibtex</a>

        <div style="white-space: pre-wrap; display: none;" class="bib">
          @InProceedings{Dwibedi_2018_CVPR_Workshops,
          author = {Dwibedi, Debidatta and Sermanet, Pierre and Tompson, Jonathan},
          title = {Temporal Reasoning in Videos Using Convolutional Gated Recurrent Units},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
          month = {June},
          year = {2018}
        }

      </div>

    </div>
  </td>
</tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/mftcn_mlpc_paper.pdf"><img src="assets/mftcn_sampling.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/mftcn_mlpc_paper.pdf"><heading>Self-Supervised Representation Learning for Continuous Control</heading></a><br>
        <em><a href="http://www.cs.unm.edu/amprg/Workshops/MLPC18/schedule.html">3rd Workshop in Machine Learning in the Planning and Control of Robot Motion</a> at <a href="https://icra2018.org/">ICRA 2018</a></em>
      </p>
      <div class="paper" id="mlpc">
        <a href="assets/mftcn_mlpc_paper.pdf">paper</a> | <a href="assets/mftcn_mlpc_slides.pdf">slides</a> | <a href="assets/mftcn_mlpc_poster.pdf">poster</a>

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="theses">Theses</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/ms_thesis.pdf"><img src="assets/ms_teaser.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/ms_thesis.pdf"><heading>Synthesizing Scenes for Instance Detection</heading></a><br>
        How can we create annotated datasets without humans for tasks like object detection and pose estimation?
      </p>
      <div class="paper" id="ms">
        <a href="assets/ms_thesis.pdf">thesis</a> | <a href="assets/ms_slides.pdf">slides</a> 

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/mtech_thesis.pdf"><img src="assets/gamerules.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/mtech_thesis.pdf"><heading>Observational  Learning  of Rules  of  Games</heading></a><br>
        Can we learn the rules of a game by observing people playing them?
      </p>
      <div class="paper" id="mtech">
        <a href="assets/mtech_thesis.pdf">thesis</a> | <a href="assets/mtech_slides.pdf">slides</a> 

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="misc">Miscellaneous</h2>
    Some other unpublished work:
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/drl.png"><img src="assets/drl.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/10701_final.pdf"><heading>Playing Games with Deep Reinforcement Learning</heading></a><br>
      </p>
      <div class="paper" id="mlpc">
        <a href="assets/10701_final.pdf"> report </a> | 
        <a href="https://www.youtube.com/watch?v=TqIUM7qlDC0">video</a> 

      </div>

    </td>
  </tr>
  <tr>
    <td width="42%" valign="top"><a href="assets/pose.png"><img src="assets/pose.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/16811_project.pdf"><heading>Towards Pose Estimation of 3D Objects in Monocular Images via Keypoint Detection</heading></a><br>
      </p>

    </td>
  </tr>    

  <tr>
    <td width="42%" valign="top"><a href="assets/handnet.png"><img src="assets/handnet.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/handnet.png"><heading>HandNet: Using Faster R-CNN to Detect Hands in Egocentric Videos</heading></a><br>
      </p>

    </td>
  </tr>   

  <tr>
    <td width="42%" valign="top"><a href="https://www.youtube.com/watch?v=VKo1L9OzB2c"><img src="assets/nao.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/se367report.pdf"><heading>A Grounded Framework for Gestures and its Applications</heading></a><br>
      </p>

    </td>
  </tr>    

</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <br>
    <p align="right"><font size="2">
      <a href="https://www.cs.berkeley.edu/~barron/">this guy's webpage is awesome</a>
    </font></p>

  </td></tr>


</tbody></table>

</td>
</tr>
</tbody></table>

<script xml:space="preserve" language="JavaScript">
  hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pa13_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('cuboid_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mftcn_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('temporal_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dac_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('tcc_abs');
</script>


</body></html>